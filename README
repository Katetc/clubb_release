$Id$

************************************************************************
*                           Copyright Notice
*                         This code is (C) 2006-2011 
*         Jean-Christophe Golaz, Vincent E. Larson, Brian M. Griffin, 
*            David P. Schanen, Adam J. Smith, and Michael J. Falk.
*
*         The distribution of this code and derived works thereof 
*                      should include this notice.
*
*         Portions of this code derived from other sources (Hugh Morrison,
*         ACM TOMS, Numerical Recipes, et cetera) are the intellectual
*         property of their respective authors as noted and also subject 
*         to copyright.
************************************************************************


************************************************************************
*                     Overview of the CLUBB code 
************************************************************************

For a detailed description of the model code see:  

``A PDF-Based Model for Boundary Layer Clouds. Part I:
  Method and Model Description'' Golaz, et al. (2002)
  JAS, Vol. 59, pp. 3540--3551.

See also the ./doc/hoceqns.pdf file in the svn repository for
finer details on how the discretization was done.

The single column model executable ("clubb_standalone") runs a particular 
case (e.g. BOMEX) and outputs statistical data in either GrADS or 
netCDF format.  GrADS is both a data file format and a plotting program.
netCDF is another data file format that can be read by the GrADS plotting
program or by MATLAB.
See <http://www.unidata.ucar.edu/software/netcdf/> and 
<http://www.iges.org/grads/> for a description of these data formats.


The tuner code tunes certain parameters in a one-dimensional boundary layer 
cloud parameterization (``CLUBB''), to best fit large-eddy simulation output.  
It is not needed to run the single-column or for using CLUBB in a host model.
The parameterization is called as a subroutine ( run_clubb() ) with 
parameter values as input.

The tuner code is highly flexible.  One can vary the cases (bomex, fire, arm, 
or atex) to match; the variables to match (cloud fraction, liquid water, third
moment of vertical velocity, etc.); the altitude and times over which to match
these variables; and the parameters to tune (C1, beta, etc.). 

***********************************************************************
*                       CLUBB Quick Start Guide                       *
***********************************************************************
CLUBB is a single-column atmospheric model written in ISO Fortran 95, and
executed using scripts written in the GNU Bash scripting language.

This quick start guide has instructions for checking CLUBB out from the UWM
repositories, compiling CLUBB using the gfortran compiler, running one of the
benchmark test cases, and creating plots using the included plotgen tool.

-------------------------------------------------------
- STEP 1: OBTAINING THE CODE FROM THE REPOSITORY
-------------------------------------------------------
Checking CLUBB out requires the user to have signed up for a CLUBB account.
This can be done at http://clubb.larson-group.com/register.php and will
require a valid e-mail address.

After entering the required information in the registration page, an e-mail
will be sent to the entered e-mail address with the password for the new
CLUBB account.

Please note that each user will only need to sign up for a CLUBB account once.
After this, the same e-mail address and password combination will work to
download the CLUBB code.

With a valid CLUBB account, the CLUBB code can be checked out from its
repository using subversion (svn).  If you have not already downloaded CLUBB,
then at a bash prompt, type in the following command, all on one line:
user@computer ~$ svn co http://carson.math.uwm.edu/repos/clubb_repos/trunk 
          clubb --username EMAIL
(EMAIL is the e-mail address used to register for a CLUBB account)

After entering this command, svn will prompt for a password. The password will
have been sent via e-mail after signing up for a CLUBB account. After the
password is entered, subversion will download the CLUBB source code into the
directory specified as the last option in the svn command; in the example
above, it will be downloaded to the clubb/ directory.  On some computers,
when entering the password, the cursor may remain in the same spot
and the screen may remain blank.  Don't be confused; hit return and 
your password will be entered.  

-------------------------------------------------------
- STEP 2: COMPILING THE CLUBB SOURCE CODE
-------------------------------------------------------
Before CLUBB can be run, the source code must first be compiled. CLUBB is known
to compile properly with various Fortran 95 compilers, including g95 and
gfortran, which are both free.

To compile, change directories to the CLUBB source directory (e.g. "clubb"),
and then to the compile directory within the CLUBB source directory.

Once in the compile directory, run the compile.bash script, using the config
file corresponding to the target platform to run CLUBB on.
For instance, on a Linux machine, to compile with gfortran, use the following
command:
user@computer compile$ ./compile.bash config/linux_ia32_gfortran.bash

To run this on a MAC OS X machine, you should use the following command:

$ ./compile.bash config/macosx_x86_64_gfortran.bash

(If you want a primer on using the command line shell on Macs, see
http://developer.apple.com/library/mac/#documentation/MacOSX/Conceptual/OSX_Technology_Overview/CommandLine/CommandLine.html#//apple_ref/doc/uid/TP40001067-CH271-BBCBEAJD .)

The command line will output information relating to the compilation of CLUBB.

If you change something in the configuration and need to remove the old 
executables and object files you can type:
$ ./clean_all.bash

-------------------------------------------------------
- STEP 3: RUNNING A BENCHMARK TEST CASE
-------------------------------------------------------
Once CLUBB is compiled, the various benchmark test cases can be run.
To do this, change directories to the run_scripts directory inside the CLUBB
checkout.

Once in the run_scripts directory, CLUBB is generally run using the
run_scm.bash script. This script will run the CLUBB model for a single case,
which can be specified on the command line. For example, to run the BOMEX case:
user@computer run_scripts$ ./run_scm.bash bomex

Other cases that you may run are listed in run_scripts/RUN_CASES.  The command line will output information relating to the case as it runs.
In addition, output from the case will be placed in the output directory inside
the CLUBB checkout.

-------------------------------------------------------
- STEP 4: VIEWING THE RESULTS WITH PLOTGEN
-------------------------------------------------------
After a case has been run, the resulting output can be used to create graphs.
There are many programs available that can be used to do this, 
but one way is to use an included perl script called plotgen. 
Plotgen interfaces with MATLAB to produce high-quality plots that allow 
CLUBB's output to be easily visualized.

**************************************************************************
                                  NOTE:
MATLAB, Imagemagick, and Ghostscript are required for plotgen to function!
**************************************************************************

The plotgen.pl script is located in the postprocessing/plotgen directory
inside the CLUBB checkout. Plotgen is run by running the plotgen.pl script,
telling it where the CLUBB output directory is, and specifying the desired
plots output directory. This must be called from the postprocessing/plotgen
directory inside the CLUBB checkout.

NOTE: For Mac users in W434, you must change a line in plotgen.pl.  To do this
go into the directory that plotgen.pl is located, type in:
$ cd ~/clubb/postprocessing/plotgen
$ nano plotgen.pl
Locate the line starting with "my $MATLAB".  There should be two, find the one
with the comment at the end saying "For Macs in W434" and at the beginning of
the line remove the # symbol.  Now go to the line above it and at the beginning
of that line insert a # symbol.  Then use Control-X to exit, then "y" to save
the changes that are made and then enter to overwrite the current plotgen.
Also, at the terminal type in the following command:
$ export DYLD_LIBRARY_PATH=/usr/local/ImageMagick/lib/

Then, to generate a webpage containing plots, type, (if your output 
is in ~/clubb/output and you want the plots to go in directory ~/plots):

user@computer ~/clubb/postprocessing/plotgen$
./plotgen.pl ~/clubb/output ~/plots

The above line will use plotgen to create plots from the simulation output
in ~/clubb/output, and place the resulting plots in the ~/plots directory.  
If you want to overplot two simulations, move each set of output files to 
its own directory, e.g. ~/clubb/output/sim1 and ~/clubb/output/sim2, and
type the command

user@computer ~/clubb/postprocessing/plotgen$
./plotgen.pl ~/clubb/output/sim1 ~/clubb/output/sim2 ~/plots

To view the plots, use a web browser to view index.html in the plots directory.
user@computer ~$ firefox ~/plots/index.html

MAC Users in W434 use the following command
$ /Applications/Firefox.app/Contents/MacOS/firefox-bin ~/plots/index.html
assuming that the index.html file is in the plots directory.

Plotgen will plot only the test cases that have output; therefore, if only
the BOMEX case has been run, for example, the resulting plotgen plots will
only have BOMEX included.

-------------------------------------------------------
- SUMMARY FOR LINUX
-------------------------------------------------------

To check out, compile, and run CLUBB, and use plotgen to view the results,
follow the following steps:

- Have a valid CLUBB account
  - Sign up at: http://clubb.larson-group.com/register.php
  - This only needs to be performed once
- Check out the CLUBB code with subversion
  - svn co http://carson.math.uwm.edu/repos/clubb_repos/trunk
    clubb --username EMAIL
    (all on one line)
- Compile the CLUBB source code with a Fortran 95 compiler (e.g. gfortran)
  - ./compile.bash config/linux_ia32_gfortran.bash
    (in the clubb/compile directory)
    (replace config/linux_ia32_gfortran.bash to match the desired platform
     and compiler)
- Run a benchmark case (e.g. BOMEX)
  - ./run_scm.bash bomex
    (in the clubb/run_scripts directory)
- Run plotgen to create plots of the output from the benchmark case simulation
  - ./plotgen.pl ~/clubb/output ~/plots
    (in the clubb/postprocessing/plotgen directory, assuming CLUBB is checked
     out to ~/clubb)
    (NOTE: MATLAB, Imagemagick, and Ghostscript are required to use plotgen!)
- View the plotgen plots using a web browser
  - firefox ~/plots/index.html

***********************************************************************
*                        Using the CLUBB Model                        *
***********************************************************************

%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 1: COMPILING 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%

-----------------------------------------------------------------------
- (1.1) Building (i.e. compiling) everything:
-----------------------------------------------------------------------

The CLUBB code is written in ISO Fortran 95 and executed by scripts written in
the GNU Bash scripting language. 
The mkmf Makefile generating script and some other optional code checking
scripts are written in the Perl scripting language.
On the Microsoft Windows platform the CLUBB parameterization could be configured
and compiled using MSYS or Cygwin with G95, but we have not tested this sort 
of configuration.

When compiling CLUBB to a new platform there are sometimes portability 
difficulties that arise, usually from different Fortran compilers.  When 
possible it may be helpful to look at similar platforms in the compile/config 
directory.  Another useful troubleshooting technique to examine the 
configuration of other Fortran projects (e.g. WRF) on the same platform.  If you
manage to compile CLUBB on a new configuration and would like to share it with
other users feel free to send us an email with your configuration attached.

We mainly use the G95 compiler on Intel x64 processors running Redhat 
Enterprise 5.  G95 <http://www.g95.org/> has been tested on SPARC & x86 Solaris,
x64 & x86 GNU/Linux. Other tested platforms are found in the compile/ config 
directory.

The GNU Fortran compiler (GCC) may or may not work.  The version that
comes with RHEL 5 does not.  The versions that comes with Fedora Core 11 and 
Ubuntu 8 LTS will both appear to work.

It is important to note that all these compilers use *incompatible* module
formats for the .mod files!  If you want to use different compilers on the
same system, you will need to build a different set of netCDF mod files for
each compiler and use -M or -I flags to specify their location.

In order to get similar results on differing architectures, platforms, and
compilers, initially try a conservative optimization and enable 
IEEE-754 standard style floating point math.  On x86 compatible machines 
using SSE or SSE2 is usually the best way to do this.

Requirements:
A. A Fortran 95 compiler with a complete implementation of the standard.
   The compile/config directory contains several scripts for 
   configurations we have tested.
B. GNU make (we use v3.81).
C. Perl 5 to run mkmf.
D. LAPACK & BLAS.  These provide the tri and band diagonal matrix solver
   subroutines needed by CLUBB.  Many vendors provide optimized versions of
   these routines, which may be much faster than the reference BLAS.  To be
   safe, we recommend you use a LAPACK and BLAS library compiled with the 
   same compiler you compile the CLUBB code with (E.g. don't compile LAPACK
   with GNU Fortran and CLUBB with Portland Group's Fortran)
E. GNU bash, or an equivalent POSIX compliant shell to use the run scripts.
   See <http://http://www.gnu.org/software/bash/>.

Optionally:
F. GrADS for viewing the GrADS output data.
G. netCDF >= v3.5.1;  We have not tested our code with anything older.
   If you do not use netCDF, you must remove -DNETCDF from the preprocessor
   flags, found in the compile/config/<platform>.bash file, and
   remove -lnetcdf from the LDFLAGS.
H. MATLAB or NCAR graphics for viewing the netCDF output data.

To build, perform the following three steps:
1. $ cd <CLUBB BASE DIRECTORY>/compile
   (<CLUBB BASE DIRECTORY> is the directory to which you checked out CLUBB.  
    Usually it is called "clubb" or some variant.)
2. Edit a ./config/<PLATFORM>.bash file and uncomment the corresponding 
   line in the file compile.bash. Depending on your platform you may need 
   to create a new file based on the existing configurations, and add a new
   line to compile.bash.  Add or uncomment the "source" statement
   for your <PLATFORM>.bash in the file ./compile.bash, and comment
   out the other "source" statements with a # character.

   Alternatively, compile.bash can take in an argument specifying the
   config file to use. In this case, the ./config/<PLATFORM>.bash file
   would still need to be edited/created according to your platform, but
   ./compile.bash would not need to be edited at all.

   Note that the variables libdir and bindir determine where
   your executables and libraries will end up, so make sure you set it
   to the correct location (the default is one directory up).
3. $ ./compile.bash
   if you edited compile.bash, or
   $ ./compile.bash ./config/<PLATFORM>.bash
   if you wish to pass in the config file as a parameter

The executables and Makefile will appear in <CLUBB BASE DIRECTORY>/bin 
and libraries in <CLUBB BASE DIRECTORY>/lib.  The object (.o) and 
module (.mod) files will appear in <CLUBB BASE DIRECTORY>/obj.

If you're using GNU make and have a fast parallel machine, parallel builds 
should work as well.  E.g. for 3 threads, append gmake="gmake -j 3" to the
file source'd from compile.bash.

The mkmf script may or may not generate files that are compatible with
other versions of make.

If you add a new source file to CLUBB, then in order for mkmf to be
able to find it, you will need to add the filename 
to one of the file lists in directory compile/file_list.

-----------------------------------------------------------------------
- (1.1.1) Promoting real to double precision at compile time
-----------------------------------------------------------------------

The g95 compiler allows for promotion of real variables without a "kind="
statement to double precision at compile time through the use of a compiler
flag.  This will not currently work with the tuner because the numerical recipes
routines throw compiler interface errors.  Therefore, it is necessary to modify
compile.bash and corresponding config file of the compiler being used.  Two
config files are currently set up for easy use of this promotion: 
linux_ia32_g95_optimize.bash and linux_ia32_g95_debug.bash.

To set up the bash scripts for compile time real promotion, follow these steps:
1. Edit config/linux_x86_64_g95_*.bash where * is either optimize or debug.
   Comment out the line " ARCH="-march=k8 -msse3 -mfpmath=sse" " and uncomment
   the line above with the " -r8 " flag.
2. Edit compile.bash in the clubb/compile directory.  Change l_double_precision
   to true.  Ensure that the config file edited above matches the uncommented 
   config file at the beginning of the script.
3. In the makefile section of the script, the line
   " cd $objdir; $gmake -f Make.clubb_tuner " must be commented out for the code
   to compile at double precision.
4. Compile as described in section 1.1.  Note again that the tuner code will not
   function as the numerical recipes code is excluded in this compilation.

-----------------------------------------------------------------------
- (1.2) Building (i.e. compiling) for use in a host model:
-----------------------------------------------------------------------

You do not need to build all the components if you have implemented CLUBB
in a large-scale weather or climate model and want to run the combined
model, rather than running CLUBB in standalone (single-column) mode 
as described above. 

Requirements:
A., B., C., D., & E. as above.

Build:

There are two basic ways to doing this:
-----------------------------------------------------------------------
- Method 1: Build libclubb_param.a and link it to the host model
-----------------------------------------------------------------------
Do 1, 2, & 3 as above.  Important Note: The host model, CLUBB, and ancillary 
programs such as netCDF and MPI need to be compiled using the same version 
of Fortran and with the same compiler flags.  Not using the same compiler and 
flags may cause errors and/or spurious results. 

Optionally, you can safely remove everything but libclubb_param.a from the 
"all" section of the compile.bash script if you only want to use CLUBB in 
a host model.
Then, do

$ ./compile.bash

This will build just the static library and the f90 modules.
The static library will be in <CLUBB BASE DIRECTORY>/lib, while the modules will be 
in the <CLUBB BASE DIRECTORY>/obj directory.  You will need at least the 
clubb_core.mod and constants.mod file to interface with CLUBB.

Addition by Brian:  
In addition to the above, you will have to make a reference to the CLUBB 
library from the configuration file of the host program.  Since CLUBB now uses 
the LAPACK libraries, you will also have to make reference to those. Currently, 
we do not include the LAPACK libraries with the CLUBB download.  You will have 
to find them and then download them onto your own computer if they are not
included with your operating system or compiler.  Once you have done this, you 
can reference them in a line such as the following:

-L/home/griffinb/clubb/lib -lclubb_param -llapack -lblas

If the LAPACK and BLAS libraries were compiled with GNU Fortran, you may 
need to link to the runtime libs for that with -lg2c or -lgfortran as well.

Don't forget that you will also have to make reference
to the CLUBB modules.  You can reference that with a line
such as the following:

-I/home/griffinb/clubb/obj

-----------------------------------------------------------------------
- Method 2: Use a host model's make to compile CLUBB 
-----------------------------------------------------------------------
We've used svn externals to put the CLUBB_core directory into the SAM host
model, and compile CLUBB using the SAM's Build script.  Other host models
that utilize B. Eaton's mkDepends and mkSrcFiles could be similarly
configured.  The basic method is to put the src/CLUBB_core directory into
the search path used by mkSrcfiles and mkDepends. These will create the
list of files to be used and then CLUBB should compile without a problem.
This tends to be the less complicated solution and allows you to make
changes to the CLUBB parameterization with just one compile step.
There are 3 key caveats when this method however:
1. The netCDF library still needs to be linked in if -DNETCDF is defined
for compiling CLUBB's source files.  If the host model itself uses the 
netCDF library, this shouldn't require any modifications to the linking.
2. The LAPACK and BLAS libraries still need to be linked into the host
model application.
3. The CLUBB model has files with a .F90 extension in addition to .f90;
You may need to modify mkSrcfiles if it's not searching for those.

-----------------------------------------------------------------------
- (1.2.1) Performance in a host model:
-----------------------------------------------------------------------
There are several key points to reducing the portion of runtime spent
by CLUBB in a host model.  These include:

1. Using a fast compiler with flags that work well for your computer.
  On UWM's machines, the Intel Fortran and Sun Studio compilers performed
  much better than g95.  Consult the documentation provided by the compiler
  vendor, and try to use options that optimize to your particular processor 
  and cache.  Generally, we recommend against using options that reduce 
  the precision of calculations, since they may negatively impact the 
  accuracy of your results.

2. Choose a fast implementation of the Basic Linear Algebra Subroutines 
  (abbreviated BLAS).  This is more crucial for CLUBB than for other models
  because CLUBB uses large matrix inversions. 
  For the LAPACK and BLAS libraries it is best to use the AMD Core Math Library,
  Intel Math Kernel Library, or ATLAS BLAS rather than the "reference" BLAS 
  that typically comes with GNU/Linux systems, because the former will greatly 
  improve the CLUBB code's runtime.
  Typically we've seen better performance with the AMD Core Math Library
  using AMD processors and the Intel MKL using Intel processors.
  The ATLAS version of BLAS has been carefully configured to work well with 
  both after being tuned at compile time to your specific setup.
  It is probably best to avoid the current Sun Performance Library for 
  production simulations, since it appears to have a high OpenMP overhead 
  when solving the matrices in CLUBB.

3. If the host model's timestep is less than a minute, then sub-cycle 
  the CLUBB code so that CLUBB is being called at a 60 or 120
  second timestep rather than the host model timestep.  The CLUBB code 
  uses a semi-implicit discretization, and should not require a small timestep.

4. In some implementations of CLUBB in a host model, we have enabled the CLUBB
  statistics code for a particular horizontal grid column (e.g. output the
  first column of the third row on the domain).  This is meant to be used
  for diagnostic purposes and should be disabled (l_stats = .false.) when
  the model is used for production runs.

5. Host models should call set_clubb_debug_level at initialization.  For
  production simulations, use an argument of 0 rather than 1 or 2.  This 
  disables warning messages and associated diagnostics, and will help speed 
  up the model.

6. When determining runtime occupied by CLUBB, keep in mind that the
  percent time spent in the CLUBB code will be proportional to other processes
  that are computed within the host model.  For example, in SAM-CLUBB the 
  percentage of runtime spent in the CLUBB code will be far less if there 
  large number of microphysical fields or tracers since the host model will 
  need to advect, diffuse, and apply other processes to each of them. The
  total time in CLUBB should be the same regardless of these other processes.

-----------------------------------------------------------------------
- (1.3) Making clean (for re-compiling from scratch)  
-----------------------------------------------------------------------

Occasionally, one needs to erase old executables or libraries and re-compile 
the code starting with nothing.  For instance, this may be required when 
a library or compiler is updated.  

To delete old object files (*.o), and mod (*.mod) files,
go to <CLUBB BASE DIRECTORY>/bin (where Makefile resides) and type

$ make clean

If this doesn't help, then to additionally delete everything in the binary 
and library directories, go to <CLUBB BASE DIRECTORY>/bin and type

$ make distclean

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 2: EXECUTING BASIC SIMULATIONS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

-----------------------------------------------------------------------
- (2.1) Executing a single-column (standalone) run:
-----------------------------------------------------------------------

   Before you can execute CLUBB, you must compile it (see Build section 
above.)
   <CLUBB BASE DIRECTORY> refers the the directory where clubb is installed.
   <CASE NAME> refers to the cloud case, e.g. arm, atex, bomex, etc.

   Note:
   For external users (i.e. those users not at UW-Milwaukee) cases that 
   are experimental or in testing won't be available. See run_scripts/RUN_CASES
   for a list of cases that are unreleased.

1. cd <CLUBB BASE DIRECTORY>/input/case_setups

2. Edit <CASE NAME>_model.in for each case you wish to run, or just leave 
   them as is.  This file contains inputs such as model timestep, vertical
   grid spacing, options for microphysics and radiation schemes, and so forth.  
   See the KK_microphys code for a description of Khairoutdinov and Kogan
   drizzle parameterization.
   See BUGSrad description below for a description of the interactive
   radiation scheme.
   Enabling radiation or microphysics parameterizations may increase runtime 
   considerably.

3. cd <CLUBB BASE DIRECTORY>/input/stats
   Edit a stats file you would like to use, if you would like to output to
   disk a variable that is not currently output.  A complete list of all 
   computable statistics is found in all_stats.in.  Note that CLUBB now 
   supports GrADS or netCDF output, but you can only use the clubb_tuner 
   using GrADS, due to some issues with buffered I/O.

4. $ cd <CLUBB BASE DIRECTORY>/input
   Edit tunable_parameters.in if you are an expert and wish to try to optimize 
   the solution accuracy.  The default values have been tested rigorously and 
   will work with all the current cases.

5. $ cd <CLUBB BASE DIRECTORY>/run_scripts
   $ ./run_scm.bash <CASE NAME> or
   $ ./run_scm.bash <CASE NAME> -p <PARAMETER FILE> -s <STATS FILE>

   Where the parameter file and stats file are optional arguments. The default
   is all_stats.in and tunable_parameters.in.

   The resulting data will be written in the directory clubb/output.

-----------------------------------------------------------------------
- (2.2) Explanation of CLUBB's output and input files
-----------------------------------------------------------------------

Nota bene: Our numerical output is usually in GrADS format
(http://www.iges.org/grads/).  Each output has a header, or control file 
(.ctl), and a data file (.dat).  The .ctl file is a text file that 
describes the file format, which variables are output in which order, etc.  
CLUBB also can output in netCDF (.nc) format.


Output:
------

Generated CLUBB GrADS files (in clubb/output):
  bomex_zt.dat, fire_zt.dat, arm_zt.dat, atex_zt.dat, dycoms_zt.dat,
    wangara_zt.dat, <case>_zm, <case>_sfc ...
  These are the output files generated by CLUBB.  Every time CLUBB is run, 
  these are overwritten, so if you want to prevent them
  from being erased be sure to either copy the .ctl and .dat
  files to another directory or rename them.

LES GrADS files (available only to larson group members):
  les_data/bomex_coamps_sw.ctl, les_data/wangara_rams.ctl
  FIRE, BOMEX, ARM & ATEX are some basic benchmark ``datasets'', 
  simulated by COAMPS, that we compare to CLUBB output.  BOMEX is trade-wind 
  cumulus; FIRE is marine  stratocumulus; ARM is continental cumulus; and 
  ATEX is cumulus under stratocumulus.  BOMEX, FIRE, and ATEX are statistically 
  steady-state; ARM varies over the course of a day.

Input:
-----

Note that at the begginning of a run the run scripts combine all the namelist
files into one file, named clubb.in, which is then fed into the CLUBB main code.
  
The namelist files:

  input/case_setups/bomex_model.in, fire_model.in, arm_model.in & atex_model.in.
  These files specify the standard CLUBB model parameters.  Usually these 
  do not need to be modified unless a new case is being set up.

  input/stats/all_stats.in, nobudgets_stats.in, etc.
  These files specify statistics output for each simulation.  See
  all_stats.in for a complete list of the all output supported.
  
  input_misc/tuner/error_all.in, error_<CASE NAME>.in, error_<DATE>.in.
  These specify tunable parameters, the initial spread of the simplex
  containing the tunable parameters, and which cases to "tune" for.

The randomization files (only needed for the tuner described below):

  run_scripts/generate_seed.bash, input/tuner/rand_seed.dat, bin/int2txt
  The script uses intrinsic functionality in the Linux kernel to generate
  a pseudo random seed (the .dat) used by the tuner for randomizing initial
  parameters.  This works on any operating system with a Linux style 
  /dev/random (Solaris, Tru64, etc.) as well.  The seed file is now plain text 
  text and can be edited by hand.

-----------------------------------------------------------------------
- (2.3) Plotting output from a single-column run:
-----------------------------------------------------------------------

Plotting scripts are contained in the directory postprocessing/plotgen.

If you have MATLAB and Linux, you can conveniently drive this MATLAB 
script using the bash script postprocessing/plotgen/plotgen.pl
See postprocessing/plotgen/README for more information on plotgen.pl.

Otherwise, you can view the raw CLUBB output files in GrADS or netCDF
format using a plotting program such as GrADS (http://www.iges.org/grads/).

--------------------------------------------------------------------------
- (2.4) Determining whether two CLUBB simulations produce different output:
--------------------------------------------------------------------------

There are 3 major ways to determine if output files are different, each will 
be covered briefly in this section.
1) run_bindiff_all.bash script: tests for bit-by-bit equivalence
2) find_grads_differences.m script: tests for differences up to round-off
3) plotting the outputs using plotgen: tests for visual agreement

- Comparison using run_bindiff_all.bash
This script takes two output directories as arguments and will compare the
contents of those directories to determine if the binary outputs differ.  
Note that
run_scm_all.bash must be run prior to this script to create the ouput to be
compared.  The script will compare the .ctl files and .dat file for each
case specified in run_scm_all.bash.  This is the most exact way to detect
differences between two output directories.

This script can be found in "clubb/run_scripts".

Usage: ./run_bindiff_all.bash output_directory output_directory_to_compare

- Comparison using find_grads_differences.m
This method also requires Matlab and must be run in Matlab.  This script 
does not check for an exact binary match; instead it compares outputs 
up to a specified tolerance.  It is useful for finding which output variable
differs between two files.  The strings 'begin' and 'end' can be used
as the time for the lower and upper time bounds which the script will determine
from the files.  A higher number for the tolerance corresponds to a finer tolerance.

This script can be found in "clubb/postprocessing/run_comparisons".

Usage: find_grads_differences( 'casename_to_check.ctl', 
                          lower_time_bound(or 'begin'),
	                  upper_time_bound(or 'end'), digits_of_precision)

Note that this script must be run in Matlab while in the directory where the
file resides.  Also, the path1, path2 and path3 variables will likely need to
be set to directories corresponding to the local path(s) of the files being compared.

- Comparison using plotgen
This method requires Matlab as the plots are generated through Matlab.
The basic usage below will plot 1 or more (as many as desired) specified
output directories against other data for comparison.  For more detailed
information on plotgen, including detailed usage and options see:
http://www.larson-group.com/twiki/bin/view.pl/Documentation/CarsonDoc/Plotgen3.

Usage: plotgen output_directory1 (output_directory2) ... directory_for_plots

Note that if the directory specified for the plots to be written to already
exists, a -r flag must also be used to delete all contents of that folder.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 3: FANCIER TYPES OF SIMULATIONS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

-----------------------------------------------------------------------
- (3.1) Executing a restart run:
-----------------------------------------------------------------------

After a long simulation has been performed, it is sometimes convenient to 
perform a new simulation that starts some time at the middle or end of the original 
simulation, rather than wasting time by starting again from the initial time.  
The new simulation is then called a "restart" simulation.  The restart
simulation is initialized using data saved to disk by the original
simulation at the restart time.

1.  Perform the original simulation of case <CASE NAME> and save the GrADS 
    or netCDF output files in the <CLUBB BASE DIRECTORY>/output directory. 
    These data files will be accessed to restart the simulation.  In order
    to reproduce the original simulation exactly, one must save
    non-time-averaged output from the original simulation.  That is, 
    in the _model.in file, one must set stats_tsamp to the same value 
    as stats_tout for the original simulation.

2.  Create a subdirectory in the <CLUBB BASE DIRECTORY> called "restart" and 
    move the GrADS output files to that subdirectory.

3.  Edit the following three variables at the end of the flag section of 
    the model file:

    l_restart = .true.
    restart_path = restart/<CASE NAME>
    time_restart = initial time of restart run in seconds

    Compute time_restart as (time_initial + n_out * stats_tout), where n_out
    is the number of output intervals before the restart time.

4.  Execute the run as usual from /run_scripts using 
    
    ./run_scm.bash <CASE NAME>

-----------------------------------------------------------------------
- (3.2) Executing a single-column run with fields input from LES:
-----------------------------------------------------------------------

One supported mode of running clubb is to use GrADS or netCDF data from either a
prior CLUBB run or a horizontally averaged set of data from an LES to
set some of the prognosed variables to the data set's values at each timestep.
E.g. If desired, the horizontal winds (variables um and vm in the code)
could be fixed to the COAMPS-LES value at each timestep, while the other
fields will evolve as in the standard single-column run.

Currently, we have only tested the code with GrADS data from COAMPS-LES.  Data 
from CLUBB also works, but should result in similar results to not 
using input_fields and so is less useful.

The relevant namelist files are in the input/<Model Case>_model.in files.  The
variable l_input_fields enabled the code when true, and then a separate
namelist called &setfields is used to control which variables are read in.

To execute an input fields run, you need to set the 'datafile' variable in the
setfields namelist to the location of the data files, and set 'input_'<varname>
to .true. for those fields for which you want to use a fixed value from
the LES dataset at the beginning of each timestep.

Nota bene:  The GrADS data files cannot have a time increment less than 1mn.
Therefore, when a file is output in CLUBB with a stats_tout of less than
60, the code will simply round up, which will not work for using the
resulting GrADS data file generated for an inputfields simulation.
Therefore, when l_input_fields is true, always use GrADS data output
at 1mn increments or greater.

You will need to set 'input_type' to the type of run you are using for input
to "clubb" or "coamps_les".

Then, change your directory to run_scripts and execute the run_scm.bash
as you would usually.

Note the input fields code will also work for tuning runs, if the tuner is
configured to use a <Case Name>_model.in with the input fields options
enabled.

-----------------------------------------------------------------------
- (3.3) Executing a tuning run:
-----------------------------------------------------------------------

The "tuner" code is used to optimize CLUBB's parameters in order to better match
output from a 3D large-eddy simulation (LES) model.  The optimization technique 
is the downhill-simplex method of Needler and Mead, as implemented in 
_Numerical Recipes In Fortran 90_ (amoeba.f90).  You will either need special
access to the CLUBB repository, or your own license from Numerical Recipes to
use the tuner.  In the latter case, the files that need to be placed in
the directory src/Numerical_recipes are:
amebsa.f90  nr.f90      nrutil.f90  ran1_v.f90
amoeba.f90  nrtype.f90  ran1_s.f90  ran_state.f90
You may need to separate out any USE statements delimited by a semicolon in
order to make the files work properly with the mkmf script.

Do steps 1, 2, & 3 as outlined in the standalone run.

4. Edit input_misc/tuner/error_<CASE NAME>.in or select an existing one. Note that
   there are two tuning subroutines, specified by tune_type in the 
   error_<CASE NAME>.in /stats/ namelist.  
   If tune_type = 0, then the amoeba subroutine, which implements the downhill 
   simplex algorithm, will be used.  If runtype is any other value, then amebsa, 
   a variant of amoeba which uses simulated annealing instead, is used.  A complete 
   explanation of these minimization algorithms can be found 
   in "Numerical Recipes" by Press et al., .
   Sometimes the variable names in the CLUBB output and the LES output
   will differ.  Note that when tuning against netCDF data, the file will
   need to have a .nc extension for the clubb_tuner to correctly identify
   the file as being in netCDF format.

5. You may also wish to set the debug_level to 0 in the file
   input/case_setups/<CASE_NAME>_model.in to speed up tuning.
   Doing this will also help make the output match the nightly tuning run. 
   You may need to set stats_tout to 60.0 in order to match the
   time interval of LES output data.  The tuner also supports 
   setting l_input_fields
   to .true. for for the purposes of `fixing' variables such as um or vm for
   the purposes of isolating model errors.

6. Edit run_tuner.bash to use your namelists.  Note that run_tuner.bash uses
   a customized stats file for tuning and a (possibly different) stats file 
   to run CLUBB with the optimized parameters.  The stats file used while 
   tuning is input/stats/tuning_stats.in and contains only the names of the 
   variables being tuned for; this speeds up the tuning process.  Therefore,
   you must also edit tuning_stats.in to match the variables being tuned for
   in the error_<CASE_NAME>.in file.

7. You will need to check out the clubb benchmark data from the repository, 
   create a directory in clubb called les_data, and move all files for the case
   or cases you want to run the tuner for from the les_runs subdirectory in 
   clubb_benchmark_data to les_data in clubb. (Note that simply checking out the
   benchmark data from the repository into les_data will not work properly,
   the case files must be in the top level of the les_data directory.)  This, along
   with setting debug_level to 0, will produce identical output to the nightly tests
   tuning run.

8. ./run_tuner.bash

-----------------------------------------------------------------------
- (3.3.1) Creating a RAM disk (optional)
-----------------------------------------------------------------------

One means of speeding up tuning runs is reducing the time spent writing
to the hard disk.  Most operating systems support a virtual device called
a ram disk, which is main memory that has been allocated to act as an emulated
file system.  Note that you will need system privileges to make the ram disk, 
and files copied to the ram disk are not preserved when the computer is 
powered off.

Generally:

1. Create and mount RAM disk on "output"

2. Run tuner

Linux Example
Note that you will need ram disk support compiled into your kernel, which is
typically the default on most systems.  Linux appears to be less flexible 
about when you are allowed to change the ramdisk size.

1. In grub.conf
   Append to 'kernel' line:
   kernel /vmlinuz-2.4.21-40.EL ro root=LABEL=/ ramdisk_size=262144

This sets ram disks to be 256 megabytes in size.  Note that your own system may
have other options besides the ramdisk_size.

2. $ mkfs.ext2 /dev/ram0

3. $ mount /dev/ram0 /home/dschanen/clubb/output

4. $ cd run_scripts

(Run your job)

Solaris Example
Note that these instructions only apply to Solaris 9 & 10

1. $ ramdiskadm -a clubb 256m 
Creates a virtual disk clubb that is 256 megabytes in size.

2. $ newfs /dev/ramdisk/clubb

3. $ mount /dev/ramdisk/clubb /home/dschanen/clubb/output/

4. $ cd run_scripts

(Run your job)

-----------------------------------------------------------------------
- (3.3.2) Executing an ensemble tuning run:
-----------------------------------------------------------------------

An ensemble tuning run generates a set ("ensemble") of optimal parameter
values.  This is useful if you want to see the range of parameter values that
will yield good results.

More detailed instructions of how to execute an ensemble tuning run
can be found in ens_tune/README

Notes and instructions for the CLUBB ensemble tuner
---------------------------------------------------

Go to the main CLUBB directory and follow these instructions:

1)  Copy the ens_tune directory to a new directory with a slightly different
    name, such as ens_tune_two.  From this point on, I will refer to this new
    directory as ens_tune_xyz.

2)  Enter the new directory (ens_tune_xyz).

3)  Decide which cases will be tuned for (ex. DYCOMS2 RF02 DO, DYCOMS2 RF01,
    FIRE, ARM, BOMEX, etc.).

4)  For EACH case being tuned for:

     a)  Copy the <CASE NAME>_model.in file from the input/case_setups directory 
	 to the ens_tune_xyz/ directory.  Make sure that the file is set up
         correctly.

     b)  Create/copy the <CASE NAME>_stats_tune.in file.  A sample of this file can
         be found in the ens_tune/ directory (and subsequently in the
         ens_tune_xyz/ directory if it was copied from the ens_tune/ directory).
         This file is used in the tuning process itself.  In order for this
         process to work, stats_fmt in this file must be set to 'grads'.  The
         tuner code reads the LES GrADS files and compares the results with the
         CLUBB GrADS files (which are written according the specifications 
         stated in this file).  This file MUST also contain the names of the 
	 CLUBB variables that are being tuned for.  During the process of 
	 tuning, small GrADS files will be written that contain the results for 
         ONLY the variables that are listed here.  This makes the tuning go
         faster because it is not being slowed down by the writing of
         unnecessary variables.

     c)  Copy the <CASE NAME>_stats.in file from the stats/ directory to the
         ens_tune_xyz/ directory.  Once the tuner has found the optimal value
         and the tuning process has finished, standalone CLUBB will run for
         each case that was tuned for with the values of the constants that the
         tuner found.  The statistics that it finally outputs will be directed
         according to this file.  This file produces the normal statistical
         files that one sees after a normal CLUBB run.  The main thing that 
	 might be changed is whether the final statistical output is in GrADS 
	 or in netCDF.  The user can choose either one.  The user just has to
         set stats_fmt to either 'grads' or 'netcdf'.  Of course, if the user
         desires, the sampling and output timesteps or the variables that are
         output can also be changed.

5)  Edit the error_messner_001.in file:

         The error_messner_001.in file is the same type of file as the error.in
         file in the regular CLUBB tuner.  This file must include all the 
         relevant information for EVERY case being tuned for, such as the 
	 CLUBB and LES stats files, the CLUBB run file, the vertical 
	 levels and time periods being tuned for, and the general weighting of 
         each case.  This file also must include other important factors such 
	 as the variable(s) being tuned for and the general weighting of each 
         variable.  Of course, the initial values of the constants and the 
	 amount of deviation allowed for each of the constants are also 
	 declared here.

         The runscript copies any error*.in file over to the remote nodes for
         running (as error.in).  So, the last file copied would end up being
         the file read in the running of the tuner.  Therefore, make sure that
         there is only ONE error*.in file in the ens_tune_xyz directory when
         you start the ensemble tuning run.

6)  Edit the mytuner_sequential.bash file:

         Only the Global variables section of this file needs to be edited.
         There are 7 global variables to edit:

         a) EXPERIMENTS:  List the names of the cases that you are tuning for.
                          This should match the <CASE NAME> model, stats, and
                          stats_tune files that were copied or created and the
                          listing of cases that is found in error*.in

         b) ARCHIVE:  The general path to a directory to put the results in.
                      Usually, many specific results subdirectories reside
                      within this declared main directory.

         c) CASE:  The subdirectory within ARCHIVE to put this tuning runs
                   specific results in.

         d) CLUBB:  The path to the CLUBB main level directory.  This is the
                  directory above ens_tune_xyz, where the tuner is being run
                  from.

         f) MEMBERMAX:  The total number of tuning members to run.  

	 g) ENSEMBLE_DIR: The directory that the tuner script is located in.	

         Whenever you are setting up a run, you need to take into account the
         amount of disk space you will need.  You need to take into account
         the size of all the LES data files for each case you are tuning for.
         You also need to take into account the size of the binary
         (executable) files that you will need.  These files include clubb_tuner,
         clubb_standalone, and int2txt.  Finally, you need to know the size of
         the CLUBB output files for each of the cases you are tuning for.  These
         files will be generated when the individual tuning iterations are
         done.  One set of the CLUBB output files will be generated for each
         tuning iteration that you run.

         For example, let's say that the executable files sum to about 25 MB.
         You decide to tune for four cases -- ARM, BOMEX, DYCOMS2 RF02 DO, and
         DYCOMS2 RF01.  Let's say that the LES data files for these four cases
         sum to about 200 MB.  So, that's already 225 MB of space needed on
         EACH NODE used in the tuning run.  Now, let's say that the netCDF
         output files for the CLUBB results for these four cases sum to about
         150 MB.  However, you are running 25 iterations of the tuner on EACH
         NODE.  So, you will be producing those 150 MB worth of CLUBB files 25
         times over.  The total space needed on the node is then:

         25 MB + 200 MB + (150 MB/iteration)*(25 iterations) = 3975 MB

         So, about 4 GB of available disk space would be needed on each node
         in order to complete the run.  You would have to check the available
         disk space in the home directory on every node that you are using in
         order to make sure that EACH NODE has enough space.

         Say that you were running the above ensemble tuning run on nodes
         1-10.  That would yield a grand total of 250 iterations of the CLUBB
         tuner.  The first 10 runs would be launched (one on each node).  When
         ALL of them are completed, then the next set of 10 runs are launched,
         and so on and so on.  When the last set is finally complete, all the
         CLUBB results are copied back to Messner, and then deleted off the
         remote tom nodes.  The amount of disk space you would need on Messner
         is:

         (150 MB/iteration)*(250 iterations) = 37500 MB

         So, you would need almost 40 GB of available space on Messner to hold
         all the results.

7)  Starting the ensemble tuning run:

         A simple ./mytuner_sequential.bash would start the tuning run.  However,
         that would require leaving a session open until the job is finished
         (which could take days).  Therefore, the special atjob.bash script
         has been created in order to have the job run in the background.
         The proper command is:  at now -f ./atjob.bash

8)  Sorting the results:

         The script sortresults.bash will send output to the screen that
         orders the results by value of the cost function, from lowest value
         (= lowest error) to highest value.  The iteration number is listed
         along with the cost function value.  This script also produces a
         text file called results.txt.  However, this file orders the output
         according to iteration number, rather than from best to worst.

         Usually, only the top so many values produce good results.  It is
         best to look at the results for the top handful of tuning iterations.
         First, to see if they look good for all the cases that were tuned for,
         and then to see if they look good for all the cases we have in CLUBB.

9)  Analyzing the results:

	 The python script analyze_tuner.py will create a scatter plot,
	 box plot and Plotgen plots of the ensemble members.  This can 
	 help you see if the members with lower cost functions actually
	 look good, or if they tuned to an undesired result.

-----------------------------------------------------------------------
- (3.4) Executing a Jacobian analysis:
-----------------------------------------------------------------------

A Jacobian analysis determines how sensitive the model results are to a change
in a parameter value.

1. $ cd ../run_scripts

2. Edit ../input_misc/jacobian.in. 

   Note that choosing a high delta_factor may make the model
   crash, which will result in no data (results for that term will come
   back as NaN).

3. $  ./run_jacobian.bash <CASE NAME> [PARAMETER FILE] [STATS FILE]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 4: "NON-CORE" CLUBB CODE
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

------------------------------------------------------------------------
- (4.1) The passive scalar code
------------------------------------------------------------------------

The CLUBB code can be run with additional non-interactive scalars.
The scalars in the code provide a generalized way of simulating a 
passive scalar in the atmosphere (e.g. carbon dioxide) 

By default CLUBB is configured to run without any passive scalars.  To use 
this option, you must modify the input/case_setups/<CASE NAME>_model.in
so that sclr_dim is equal to the number of passive scalars.  You will also 
need to set variable ii<SCALAR NAME> to be the index of the array which 
contains the passive scalar.  For example, iisclr_CO2 is setup in the COBRA
case to be the index containing carbon dioxide.  New passive scalars with
different fluxes and forcings would need to be added to the code.
The initial sounding can be done at run time in <CASE NAME>_sclr_sounding
file, but large scale forcing and surface fluxes for these passive scalars 
must be configured in the clubb_driver code and handled at compile time.
To output the scalar fields from a CLUBB simulation, be sure to include 
sclr1m, sclr1p2, etc. your stats file. See input/stats/all_stats.in for a
complete list (commented out by default).

Currently the code contains eddy-diffusivity scalar code (edsclr1m, edsclr2m)
and the more sophisticated high-order scalars (sclr1m, sclr2m).  Both use two
dimensional arrays, but the code and results for each is separated.
The high-order scalars require a tolerance, called `sclrtol' to set in the
namelist file. Generally this value should be larger than machine epsilon.

Initially, the scalar arrays were configured to contain two vertical columns
containing copies of thl and rt for testing purposes.
The code is sufficiently general that an arbitrary number of scalars can be
added with a small number of modifications.

The Namelists:

Within the existing <CASE NAME>_sclr_sounding.in for each run, a sounding for 
the scalar variable must be added.  See input/case_setups/cobra_sclr_sounding.in
for an example.

Finally, if you wish to see the results of your calculations, you will need to
append the names of variabless to the vars_zt and var_zm portion of the 
&stats namelist.  The file scalars_stats.in has this done already.
The variables follow the convention of having the index number appended after 
the sclr portion of their name.  For example, the first scalar mean is 'sclr1m',
and the second is 'sclr2m'.  These and their forcings are all that occurs in
the zt file, the rest (e.g. variance, flux) all occur in the zm file.

Note that the scalars can be used in a host model as well.  See SAM-CLUBB for
an example of how to do this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 5: CODE CONTRIBUTED BY EXTERNAL RESEARCH GROUPS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

------------------------------------------------------------------------
- (5.1) The BUGSrad Radiation scheme
------------------------------------------------------------------------

  This is an optional interactive radiation scheme, developed separately from
  CLUBB by Stephens, et al. The code used in CLUBB was obtained from Norm Wood 
  on 2004/07/10.
  When enabled, the analytic computation normally
  used for radiation is disabled.  BUGSrad is enabled in the 
  input/case_setups/<CASE NAME>_model.in file by setting rad_scheme = "bugsrad".
  
  Some important pre-processor directives for BUGSrad follow:

  -Dradoffline: BUGSrad was originally developed both for use within the CSU GCM
  (BUGS) and for use as a standalone (single-column) radiative transfer code.
  This preprocessor flag configures the code to operate as a standalone code,
  independent of the CSU GCM.  You must compile CLUBB with the -Dradoffline 
  preprocessor flag.

  -Dnooverlap: Treats clouds as horizontally homogeneous, with no partial
  cloudiness.  Otherwise, the default overlap assumption is maximum/random.

  -DUSE_BUGSrad_ocast_random:  This is an overlap treatment that should probably
  be considered experimental.  It's similar to maximum/random but in testing 
  CSU that performed, it showed somewhat better agreement (in terms of fluxes) 
  with realistic clouds.  It's not been extensively tested, though.

  BUGSrad requires input of *within-cloud* liquid water, ice, etc.
  qcwl is the in-cloud mixing ratio (the same is true for qcil, qril etc.,
  --- all are the in-cloud values).  This is what the default maximum/random
  overlap treatment expects.  If cloud overlap is turned off by using 
  the Dnooverlap flag, these mixing ratios get diluted according to the 
  layer cloud fraction.

  BUGSrad allows the output of the following variables:

  Momentum grid:
  Frad, Frad_SW, Frad_LW:  Radiative Flux; Short-wave/Long-wave component;

  Thermodynamic grid:
  radht, radht_SW, radht_LW:  Radiative Heat; Short-wave/Long-wave component;

  The thlm_forcing variable will also have radht added to it.  This is an
  explicit contribution to the thlm calculation.

  Note that for most simulations SW and LW components are not calculated 
  without using BUGSrad.

------------------------------------------------------------------------
- (5.2) The COAMPS microphysics scheme
------------------------------------------------------------------------

     COAMPS microphysics is a single-moment scheme that includes
  the following hydrometeor categories: cloud water, rain, cloud ice,
  snow, and graupel.  It is based on Rutledge and Hobbs (1983).

     COAMPS was developed by the Naval Research Laboratory,
  Monterey, California.  COAMPS is a registered trademark of the
  Naval Research Laboratory.

     By default, COAMPS microphysics is not distributed with the code 
  outside of UWM.  If you are interested in this code, please contact
  James Doyle at the Naval Research Laboratory.

------------------------------------------------------------------------
- (5.3) The Morrison microphysics scheme
------------------------------------------------------------------------

Morrison microphysics is a double-moment scheme that can predict mixing
ratio's and number concentrations for cloud water, rain, cloud ice,
snow, and graupel.  Details of its implementation may be found in:

H. Morrison, J. A. Curry, and V. I. Khvorostyanov, 2005: A new double-
moment microphysics scheme for application in cloud and climate models. 
Part 1: Description. J. Atmos. Sci., 62, 1665–1677.

You can enable the Morrison scheme by setting micro_scheme = "morrison"
in the &microphysics_setting namelist.

Some useful &microphysics_setting namelist flags for the 
Morrison microphysics code:

-----------------------------|--------------------------------------------------
Logical flag                 |  If true then:

  l_ice_micro      = .true.   Calculate ice mixing ratio.

  l_graupel        = .true.   Calculate graupel mixing ratio.

  l_hail           = .true.   Dense precipitating ice is hail rather than 
    graupel.

  l_seifert_beheng = .true.   Use Seifert and Beheng (2001) rain scheme,
    rather than Khairoutdinov and Kogen (2000).

  l_predictnc      = .true.   Prognose droplet number concentration
    (rather than specify it with Ncm_initial).

  l_specify_aerosol = .true.  Use lognormal aerosol size distribution to
    derive ccn spectra, rather than power-law.

  l_subgrid_w       = .true.  Use the SGS calculation of the std of w to
    determine cloud droplet activation.

  l_arctic_nucl     = .true.  Use the MPACE observations rather than the
    Rasmussen et al., 2002 ice nucleation formula.

  l_cloud_edge_activation = .true. Assume droplet activation at lateral cloud
    edges due to unresolved entrainment and mixing dominates. 

  l_fix_pgam  = .true.  Fix the value of pgam (exponent of cloud water's gamma
    dist.) from Martin et al. (1994).
-----------------------------|--------------------------------------------------

  Other parameters:

  Ncm_initial      =  #/cc    Either the initial value of cloud droplet number
    concentration, or the constant value, depending on l_predictnc.

  pgam_fixed  = #  Value to use for a fixed pgam

  aer_n1,n2   = #/cm3  Aerosol concentration

  aer_sig1,sig2 = # Standard deviation of aerosol size distribution

  aer_rm1,rm2   = μ Mean geometric radius 

The budgets for the mixing ratio and number concentrations when using 
the Morrison microphysics are as follows:

  For rain water mixing ratio and number concentration:
  rrainm_bt = rrainm_ma + rrainm_dff + rrainm_mc + rrainm_cond_adj + rrainm_cl
  Nrm_bt    = Nrm_ma + Nrm_dff + Nrm_mc + Nrm_cond_adj + Nrm_cl

  All other species should be as follows:
  xm_bt = xm_ma + xm_dff + xm_mc + xm_cl

  Note that unlike the other two schemes, the Morrison scheme contains its 
  own sedimentation code.  Therefore, xm_mc includes xm_sd in addition to
  local processes and clipping.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 6: CONTRIBUTING CODE CHANGES TO UNIV WISC --- MILWAUKEE
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

------------------------------------------------------------------------
- (6.1) Contributing code changes 
------------------------------------------------------------------------

If you have changes that you'd like to see included in the repository version
of CLUBB, please feel free to contribute them.  We'll gladly review your 
changes and consider them for inclusion.  

To contribute the changes, type 'svn update' to merge the latest repository
version of CLUBB with your local version, and then create a file containing 
the differences between the repository and your version:

$ svn diff > patchfile

Then email us patchfile.  See the svn book online for more details about svn.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 7: VARIABLES NAMES IN CLUBB
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

------------------------------------------------------------------------
- (7.1) Generic naming conventions 
------------------------------------------------------------------------

Certain letters in CLUBB's variable names have special meanings (usually):

m   = horizontal mean.          E.g., wm is the mean vertical velocity.
p   = prime or perturbation.    E.g., wp2 is the variance of vertical velocity  
r   = mixing ratio.             E.g., rtm is the mean total water mixing ratio.
th  = theta or potential temp.  E.g., thlm is the liquid water pot. temp.                 
_zm = interpolated to the       E.g., rho_zm is the density on the     
      momentum grid.                momentum grid.
_zt = interpolated to the       E.g., wp2_zt is wp2 on the
      thermodynamic grid.           thermodynamic grid

-------------------------------------------------------------------------                            
- (7.2) Some important variables in CLUBB:
-------------------------------------------------------------------------

gr         = <grid>    = Main grid class reference for CLUBB                   [-]
gr%nnzp    = Number of vertical levels in grid                                 [-]
gr%zm      = Grid levels (altitudes) of "momentum" variables (wp2, etc.)       [m]
gr%zt      = Grid levels (altitudes) of "thermodynamic" variables (rtm, etc.)  [m]
thlm       = <thl>     = Liquid water potential temperature                    [K]
rtm        = <rt>      = Total water mixing ratio                              [kg/kg]
cloud_frac = Cloud fraction                                                    [-]
rcm        = <rc>      = Cloud water mixing ratio                              [kg/kg]
wp2        = <w'2>     = Variance of vertical velocity                         [m^2/s^2]
wp3        = <w'3>     = Third order moment of vertical velocity               [m^3/s^3]
wpthlp     = <w'thl'>  = Vertical turbulent flux of thl                        [(m/s) K] 
wprtp      = <w'rt'>   = Vertical turbulent flux of rt                         [(kg/kg) (m/s)]
thlp2      = <thl'2>   = Variance of thl                                       [K^2]
rtp2       = <rt'2>    = Variance of rt                                        [(kg/kg)^2]
rtpthlp    = <rt'thl'> = Covariance of rt and thl                              [(kg/kg) K]
wm         = <w>       = Mean vertical wind                                    [m/s]
um         = <u>       = Mean east-west wind                                   [m/s]
vm         = <v>       = Mean north-south wind                                 [m/s]
upwp       = <u'w'>    = Covariance of u and w                                 [m^2/s^2]
vpwp       = <v'w'>    = Covariance of v and w                                 [m^2/s^2]
up2        = <u'2>     = Variance of u                                         [m^2/s^2]
vp2        = <v'2>     = Variance of v                                         [m^2/s^2]
rrainm     = Mean rainwater mixing ratio                                       [kg/kg]

****************************************************************************


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CHAPTER 8: TROUBLESHOOTING
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

1.  If you have trouble compiling, sometimes it helps to delete
old object files and re-compile from scratch.  E.g., to delete 
everything in the binary and library directories, 
go to <CLUBB BASE DIRECTORY>/bin and type

$ make distclean

